name: vqfr_fos_v
# model
model_type: VQFRModel
fidelity_ratio: 0.1

num_gpu: 1 # set num_gpu: 0 for cpu mode
save_suffix: ''
# FaceRestoreHelper
helper:
  upscale_factor: 1
  save_ext: png
  use_parse: True

datasets:
  props:
    mean: [0.5, 0.5, 0.5]
    std: [0.5, 0.5, 0.5]
    face_size: 512

  test_v_wild:  # the 1st test dataset
    name: fos_v
    type: FosVideoDataset
    dataroot_lq: data/user_study/fos_v_aligned
    aligned: true

    seq_length: 1
    metrics: [fid_folder, serfiq, maniqa, faceqnet]

# validation settings
val:
  metrics:
    niqe:
      type: calculate_niqe
    fid_folder: # metric name, can be arbitrary
      type: calculate_fid_folder
      fid_stats: weights/stats/inception_FFHQ_512.pth
    psnr: # metric name, can be arbitrary
      type: calculate_psnr
      crop_border: 0
      test_y_channel: false
    ssim:
      type: calculate_ssim
      crop_border: 0
      test_y_channel: false

# network related (e.g. structures
network_g:
  # The model same with the released version from CodeFormer
  type: VQFRv2

  # the benchmark used the default setting
  base_channels: 64
  channel_multipliers: [1, 2, 2, 4, 4, 8]
  num_enc_blocks: 2
  use_enc_attention: True
  num_dec_blocks: 2
  use_dec_attention: True
  code_dim: 256
  inpfeat_dim: 32
  align_opt: {
      'cond_channels': 32,
      'deformable_groups': 4
  }
  code_selection_mode: 'Predict'  # Predict/Nearest
  quantizer_opt: {
      'type': 'L2VectorQuantizer',
      'num_code': 1024,
      'code_dim': 256,
      'spatial_size': [16, 16]
  }

# path
path:
  # model
  pretrain_network: VQFR_v2.pth
  pretrain_model_url: https://github.com/TencentARC/VQFR/releases/download/v2.0.0/VQFR_v2.pth